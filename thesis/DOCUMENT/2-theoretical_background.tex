\chapter{Θεωρητικό υπόβαθρο}
\label{ch:theoreteical_background}

\section{Νευρωνικό δίκτυο}

\section{\tl{ReLU}}
Η συνάρτηση \tl{ReLU (Rectified Linear Unit)} είναι μια γραμμική συνάρτηση η οποία είναι τροποποιημένη στο μέρος της αρνητικής εισόδου της ώστε να μηδενίζει την έξοδο. Η χρήση της είναι συνήθης στην δομή των νευρωνικών δικτύων, λόγω της ιδιότητας της να αποκόπτει αρνητικές τιμές κατά την διάδοση των σημάτων μεταξύ των κόμβων-νευρώνων, κάτι που επιτρέπει την ομαλή εκπαίδευση του.

\section{\tl{2D Convolutional Layers - Filters Sizes}}
Τα συνελικτικά επίπεδα είναι το χαρακτηριστικό μέρος των συνελικτικών δικτύων, όπως προδίδει το όνομα τους αποτελούν τμήματα τα οποία μετασχηματίζουν και εξάγουν από την είσοδο πληροφορία με βάση την εφαρμογή *τοπικών φίλτρων*. Η διαδικασία αυτή επιτρέπει την εξαγωγή χρήσιμης πληροφορίας προς τα επόμενα επίπεδα για την ανίχνευση πιθανών μοτίβων ενώ ταυτόχρονα δεν απαιτεί το μέγεθος των πυκνών \tl{dense} επιπέδων καθώς αφορά πράξεις ενός σημείου της εισόδου με ορισμένα γειτονικά σημεία και όχι την μέθοδο πράξεων σημεία όλα προς όλα.

\section{\tl{Dense Layers}}
Η χρήση των πυκνών επιπέδων είναι απαραίτητη για την αποτελεσματική πρόβλεψη ενός μοντέλου. Η τοποθέτηση τους στην δομή του μοντέλου ωστόσο προτιμάται να γίνεται σε σημεία όπου η διαστάσεις του προηγούμενου επιπέδου είναι σχετικά μικρές και η επεξεργασία του συγκεκριμένου επιπέδου αφορά την εκτίμηση της προηγουμένως επεξεργασμένης εισόδου.

\section{Μέθοδος εξαγωγής \tl{Spectrogram}}

\section{\tl{Absorbances-Savintzky Golay}}

\section{Υποδειγματοληψία \tl{Max Pooling}}
Τα επίπεδα \tl{Max Pooling} είναι ένα σημαντικά για τη δομή ενός συνελικτικού νευρωνικού δικτύου καθώς μειώνουν τις διαστάσεις την διάσταση της εισόδου μειώνοντας έτσι τον αριθμό των παραμέτρων του νευρωνικού δικτύου. Ταυτόχρονα η πληροφορία από τα σημεία της εισόδου που έχουν ενεργοποιηθεί δεν χάνεται. 

\section{Αρχικοποίηση Νευρωνικών Δικτύων}

Κατά την αρχικοποίηση ενός νευρωνικού δικτύου, η εκχώρηση των κατάλληλων βαρών κάθε κόμβου παίζει σημαντικό ρόλο στην μετέπειτα δυνατότητα αυτού να τροποποιείται ώστε να βελτιώνει την απόδοση του κατά την διαδικασία της εκπαίδευσης.

\section{Μέθοδοι αρχικοποίησης βαρών συνελικτικών νευρωνικών δικτύων (αρχικοποιητές)}
Κατά την πρώτη υλοποίηση χρησιμοποιήθηκε ο αρχικοποιητής των \tl{kernel *random\_uniform*}

\section{Μετρικές αξιολόγησης}
$RMSE$, ${R^2}$, $RPIQ$ 

\section{Μετρικές εκπαίδευσης}
\tl{Mean Square Error}

\section{\tl{Optimizers}}
\tl{Adam Optimizer}

