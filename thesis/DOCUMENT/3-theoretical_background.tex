\chapter{Θεωρητικό υπόβαθρο}
\label{ch:theoreteical_background}

\section{Νευρωνικά δίκτυα}
Το νευρωνικό δίκτυο με την αφηρημένη έννοια του είναι μια δομή η οποία θα μπορούσε να χαρακτηριστεί ως ένας μηχανισμός αριθμητικών πράξεων με συνήθως περισσότερες από μια εισόδους και μια ή περισσότερες εξόδους. Ένα νευρωνικό δίκτυο αποτελείται από τους κόμβους του (\tl{nodes}) οι οποίοι ενώνονται μέσω των νευρώνων του (\tl{tensors}). Το σύνολο των κόμβων οι οποίοι συνδέονται την ίδια ομαδα συνάψεων λέγεται επίπεδο (\tl{layer}). Η στατική μορφή ενός νευρωνικού δικτύου ως μηχανισμό εξαγωγής αποτελεσμάτων αριθμητικών πράξεων ή αλλιώς η πρόβλεψη ενός μοντέλου αποτελεί μια απλή διαδικασία όπου το αποτέλεσμα κάθε κόμβου πολλαπλασιάζεται με τα βάρη των συνάψεων \tl{tensors} και εισέρχεται στους κόμβους του επόμενου επιπέδου σαν είσοδος.

Για την σωστή λειτουργία ενός νευρωνικού δικτύου είναι απαραίτητοι ορισμένοι επιπλέον μηχανισμοί οι οποίοι μετασχηματίζουν τις τιμές εξόδου των κόμβων, αυτές είναι οι συναρτήσεις ενεργοποίησης κάποιες από τις οποίες θα παρουσιαστούν παρακάτω.

\subsection{\tl{ReLU}}
Η συνάρτηση \tl{ReLU (Rectified Linear Unit)} είναι μια γραμμική συνάρτηση η οποία είναι τροποποιημένη στο μέρος της αρνητικής εισόδου της ώστε να μηδενίζει την έξοδο. Η χρήση της είναι συνήθης στην δομή των νευρωνικών δικτύων, λόγω της ιδιότητας της να αποκόπτει αρνητικές τιμές κατά την διάδοση των σημάτων μεταξύ των κόμβων-νευρώνων, κάτι που επιτρέπει την ομαλή εκπαίδευση του.

\subsection{\tl{Linear}}

\section{Συνελικτικά Νευρωνικά Δίκτυα}
Τα Συνελικτικά Νευρωνικά Δίκτυα αποτελούν μια μορφή των νευρωνικών ...

Επίσης χρησιμοποιούνται ορισμένοι ακόμη τύποι επιπέδων όπως για παράδειγμα την τροποποίηση των διαστάσεων της εισόδου με συγκέντρωση μεγίστων (\tl{max pooling})

\subsection{\tl{2D Convolutional Layers - Filters Sizes}}
Τα συνελικτικά επίπεδα είναι το χαρακτηριστικό μέρος των συνελικτικών δικτύων, όπως προδίδει το όνομα τους αποτελούν τμήματα τα οποία μετασχηματίζουν και εξάγουν από την είσοδο πληροφορία με βάση την εφαρμογή *τοπικών φίλτρων*. Η διαδικασία αυτή επιτρέπει την εξαγωγή χρήσιμης πληροφορίας προς τα επόμενα επίπεδα για την ανίχνευση πιθανών μοτίβων ενώ ταυτόχρονα δεν απαιτεί το μέγεθος των πυκνών \tl{dense} επιπέδων καθώς αφορά πράξεις ενός σημείου της εισόδου με ορισμένα γειτονικά σημεία και όχι την μέθοδο πράξεων σημεία όλα προς όλα.

Στην υλοποίηση που εξετάζεται χρησιμοποιούνται συνελικτικά επίπεδα 2 διαστάσεων καθώς η είσοδος του δικτύου όπως θα αναφερθεί προκύπτει ως εικόνα ενός καναλιού έπειτα από μετασχηματισμό του αρχικού σήματος.

\subsection{\tl{Dense Layers}}
Η χρήση των πυκνών επιπέδων είναι απαραίτητη για την αποτελεσματική πρόβλεψη ενός μοντέλου. Η τοποθέτηση τους στην δομή του μοντέλου ωστόσο προτιμάται να γίνεται σε σημεία όπου η διαστάσεις του προηγούμενου επιπέδου είναι σχετικά μικρές και η επεξεργασία του συγκεκριμένου επιπέδου αφορά την εκτίμηση της προηγουμένως επεξεργασμένης εισόδου.

\subsection{Συγκέντρωση μεγίστων \tl{Max Pooling}}
Τα επίπεδα \tl{Max Pooling} είναι ένα σημαντικά για τη δομή ενός συνελικτικού νευρωνικού δικτύου καθώς μειώνουν τις διαστάσεις την διάσταση της εισόδου μειώνοντας έτσι τον αριθμό των παραμέτρων του νευρωνικού δικτύου. Ταυτόχρονα η πληροφορία από τα σημεία της εισόδου που έχουν ενεργοποιηθεί δεν χάνεται. 

\subsection{Αρχικοποίηση Νευρωνικών Δικτύων}

Κατά την αρχικοποίηση ενός νευρωνικού δικτύου, η εκχώρηση των κατάλληλων βαρών κάθε κόμβου παίζει σημαντικό ρόλο στην μετέπειτα δυνατότητα αυτού να τροποποιείται ώστε να βελτιώνει την απόδοση του κατά την διαδικασία της εκπαίδευσης.

\subsection{Μέθοδοι αρχικοποίησης βαρών συνελικτικών νευρωνικών δικτύων (αρχικοποιητές)}
Κατά την πρώτη υλοποίηση χρησιμοποιήθηκε ο αρχικοποιητής των \tl{kernel *random\_uniform*}

\subsection{\tl{Optimizers}}
\tl{Adam Optimizer, Nadam}


\section{Μέθοδος εξαγωγής \tl{Spectrogram}}

\subsection{\tl{Absorbances-Savintzky Golay}}


\section{Μετρικές αξιολόγησης}
$RMSE$, ${R^2}$, $RPIQ$ 

\section{Μετρικές εκπαίδευσης}
\tl{Mean Square Error}
